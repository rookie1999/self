import sys
import os
import torch
import torch.nn.functional as F
import matplotlib

# å¼ºåˆ¶ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé˜²æ­¢ AttributeError
matplotlib.use('TKAgg')
import matplotlib.pyplot as plt
from matplotlib.patches import Circle
from PIL import Image
from torchvision import transforms
from safetensors.torch import load_file


class DINOv3Matcher:
    def __init__(self,
                 repo_dir,
                 model_path,
                 device='cuda' if torch.cuda.is_available() else 'cpu',
                 img_size=512):  # ã€å…³é”®ä¿®æ”¹ã€‘æ”¹ä¸º 512 (16*32)ï¼Œè§£å†³åæ ‡æ¼‚ç§»é—®é¢˜
        """
        åˆå§‹åŒ– DINOv3 åŒ¹é…å™¨
        """
        self.device = device
        self.img_size = img_size
        self.patch_size = 16  # ViT-B-16 å›ºå®šä¸º 16

        print(f"ğŸš€ åˆå§‹åŒ–... è®¾å¤‡: {self.device}")

        # 1. åŠ è½½æ¶æ„
        self.model = self._load_architecture(repo_dir)
        self.model.to(self.device)
        self.model.eval()

        # 2. åŠ è½½æƒé‡
        self._load_weights(model_path)

        # 3. é¢„å¤„ç† (ä½¿ç”¨ img_size=512)
        self.transform = transforms.Compose([
            transforms.Resize((self.img_size, self.img_size), interpolation=3),  # Bicubic
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ])

    def _load_architecture(self, repo_dir):
        if repo_dir not in sys.path:
            sys.path.append(repo_dir)

        # å°è¯•å¯¼å…¥ ViT å®šä¹‰
        try:
            from dinov3.models.vision_transformer import vit_base
        except ImportError:
            try:
                from models.vision_transformer import vit_base
            except ImportError as e:
                raise ImportError(f"æ— æ³•æ‰¾åˆ°æ¨¡å‹å®šä¹‰ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {repo_dir}")

        # åˆå§‹åŒ–æ¨¡å‹ (å¼€å¯ registers)
        try:
            model = vit_base(
                img_size=self.img_size,
                patch_size=16,
                num_register_tokens=4  # DINOv3 é»˜è®¤æœ‰4ä¸ª registers
            )
            self.has_registers = True
            print("âœ… æ¨¡å‹æ¶æ„åˆ›å»ºæˆåŠŸ (å« Register Tokens)")
        except TypeError:
            print("âš ï¸ æœ¬åœ°ä»£ç ä¸æ”¯æŒ Register Tokensï¼Œé™çº§åŠ è½½ã€‚")
            model = vit_base(img_size=self.img_size, patch_size=16)
            self.has_registers = False

        return model

    def _load_weights(self, model_path):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æƒé‡: {model_path}")

        print(f"ğŸ“¦ åŠ è½½æƒé‡: {model_path}")
        state_dict = load_file(model_path)
        new_dict = {}
        model_params = dict(self.model.named_parameters())

        for k, v in state_dict.items():
            # 1. é”®åæ¸…æ´—
            k = k.replace('module.', '').replace('backbone.', '')
            k = k.replace('embeddings.patch_embeddings.weight', 'patch_embed.proj.weight')
            k = k.replace('embeddings.patch_embeddings.bias', 'patch_embed.proj.bias')
            k = k.replace('embeddings.cls_token', 'cls_token')
            k = k.replace('embeddings.mask_token', 'mask_token')
            k = k.replace('embeddings.position_embeddings', 'pos_embed')
            k = k.replace('embeddings.register_tokens', 'register_tokens')
            k = k.replace('encoder.layers.', 'blocks.')
            k = k.replace('encoder.norm.', 'norm.')

            # 2. æ™ºèƒ½ç»´åº¦é€‚é…
            if k in model_params:
                target_shape = model_params[k].shape
                # ä¿®å¤ [1, 768] vs [1, 1, 768]
                if v.ndim != len(target_shape):
                    if v.ndim == 2 and len(target_shape) == 3:
                        v = v.unsqueeze(1)
                    elif v.ndim == 3 and len(target_shape) == 2:
                        v = v.squeeze(1)

            # 3. Pos Embed æ’å€¼ (å…³é”®ï¼šé€‚é… 512 åˆ†è¾¨ç‡)
            if k == 'pos_embed' and v.shape != model_params[k].shape:
                print(f"ğŸ”„ è‡ªåŠ¨è°ƒæ•´ pos_embed: {v.shape} -> {model_params[k].shape}")
                v = self._resize_pos_embed(v, model_params[k].shape)

            new_dict[k] = v

        self.model.load_state_dict(new_dict, strict=False)
        print("âœ… æƒé‡åŠ è½½å®Œæˆ")

    def _resize_pos_embed(self, pos_embed, expected_shape):
        """å¯¹ä½ç½®ç¼–ç è¿›è¡ŒåŒçº¿æ€§æ’å€¼"""
        # pos_embed: [1, Total_Source_Tokens, D]
        n_special = 1 + (4 if self.has_registers else 0)  # CLS + Registers

        cls_tokens = pos_embed[:, :n_special, :]
        patch_tokens = pos_embed[:, n_special:, :]

        # Reshape to grid
        orig_size = int(patch_tokens.shape[1] ** 0.5)
        dim = patch_tokens.shape[-1]
        patch_tokens = patch_tokens.reshape(1, orig_size, orig_size, dim).permute(0, 3, 1, 2)

        # Interpolate
        target_count = expected_shape[1] - n_special
        target_size = int(target_count ** 0.5)

        patch_tokens = F.interpolate(
            patch_tokens, size=(target_size, target_size),
            mode='bicubic', align_corners=False
        )

        # Flatten back
        patch_tokens = patch_tokens.permute(0, 2, 3, 1).flatten(1, 2)
        return torch.cat((cls_tokens, patch_tokens), dim=1)

    def extract_features(self, tensor):
        """
        ä½¿ç”¨ DINOv3 åŸç”Ÿ API æå–ç‰¹å¾
        """
        with torch.inference_mode():
            # ã€ä¿®æ­£ã€‘forward_features ç›´æ¥è¿”å›å­—å…¸ (å½“è¾“å…¥ä¸ºå•å¼ å›¾æ—¶)
            out_dict = self.model.forward_features(tensor)

            # æ£€æŸ¥æ˜¯å¦æ„å¤–è¿”å›äº†åˆ—è¡¨ (é˜²å¾¡æ€§ç¼–ç¨‹)
            if isinstance(out_dict, list):
                out_dict = out_dict[0]

            # ç›´æ¥è·å– patch tokens
            if 'x_norm_patchtokens' in out_dict:
                patch_tokens = out_dict['x_norm_patchtokens']  # [B, N_Patches, D]
            else:
                # æ‰“å°å¯ç”¨é”®ä»¥å¸®åŠ©è°ƒè¯•
                raise RuntimeError(f"æœªæ‰¾åˆ°ç‰¹å¾é”®ï¼Œå¯ç”¨é”®: {out_dict.keys()}")

            # è®¡ç®—ç½‘æ ¼å¤§å°
            # å¼ºåˆ¶äº† img_size=512, patch_size=16 -> 32x32
            h = w = self.img_size // self.patch_size

            return patch_tokens, (h, w)

    def find_correspondence(self, img1_path, img2_path, query_point):
        # 1. é¢„å¤„ç†
        img1_pil, tensor1 = self.preprocess(img1_path)
        img2_pil, tensor2 = self.preprocess(img2_path)

        # 2. æå–ç‰¹å¾
        feat1, (h1, w1) = self.extract_features(tensor1)
        feat2, (h2, w2) = self.extract_features(tensor2)

        print(f"ğŸ“Š ç‰¹å¾ç½‘æ ¼å¤§å°: {h1}x{w1} (Tokenæ•°: {feat1.shape[1]})")

        # 3. åæ ‡æ˜ å°„ (Pixel -> Grid)
        orig_w, orig_h = img1_pil.size
        qx, qy = query_point

        # ä½¿ç”¨ img_size (512) è¿›è¡Œå½’ä¸€åŒ–
        grid_x = int(qx / orig_w * w1)
        grid_y = int(qy / orig_h * h1)
        grid_x = min(max(grid_x, 0), w1 - 1)
        grid_y = min(max(grid_y, 0), h1 - 1)

        query_idx = grid_y * w1 + grid_x

        # 4. è®¡ç®—ç›¸ä¼¼åº¦
        q_feat = F.normalize(feat1[0, query_idx, :].unsqueeze(0), p=2, dim=-1)
        k_feat = F.normalize(feat2[0], p=2, dim=-1)

        sim = torch.mm(q_feat, k_feat.t())
        best_idx = torch.argmax(sim).item()
        max_sim = sim[0, best_idx].item()

        # 5. åæ ‡è¿˜åŸ (Grid -> Pixel)
        target_grid_y = best_idx // w2
        target_grid_x = best_idx % w2

        target_orig_w, target_orig_h = img2_pil.size
        # æ˜ å°„å›åŸå›¾ä¸­å¿ƒç‚¹
        target_x = int((target_grid_x + 0.5) / w2 * target_orig_w)
        target_y = int((target_grid_y + 0.5) / h2 * target_orig_h)

        print(f"âœ… åŒ¹é…æˆåŠŸ: ç›¸ä¼¼åº¦ {max_sim:.3f}")
        self._visualize(img1_pil, img2_pil, query_point, (target_x, target_y))

    def preprocess(self, img_path):
        image = Image.open(img_path).convert('RGB')
        tensor = self.transform(image).unsqueeze(0).to(self.device)
        return image, tensor

    def _visualize(self, img1, img2, pt1, pt2):
        fig, axes = plt.subplots(1, 2, figsize=(16, 8))
        axes[0].imshow(img1)
        axes[0].add_patch(Circle(pt1, 10, color='red', fill=True))
        axes[0].add_patch(Circle(pt1, 30, color='red', fill=False, lw=2))
        axes[0].set_title("Source")

        axes[1].imshow(img2)
        axes[1].add_patch(Circle(pt2, 10, color='red', fill=True))
        axes[1].add_patch(Circle(pt2, 30, color='red', fill=False, lw=2))
        axes[1].set_title("Target Match")

        plt.tight_layout()
        plt.savefig("result_final.png")
        print("ğŸ–¼ï¸ ç»“æœå·²ä¿å­˜: result_final.png")


if __name__ == "__main__":
    # è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„
    REPO_DIR = "/home/benson/projects/dinov3"
    MODEL_PATH = "/home/benson/projects/second_work/modelscope/hub/models/facebook/dinov3-vitb16-pretrain-lvd1689m/model.safetensors"

    IMG1 = "pic.jpg"
    IMG2 = "pic2.jpg"
    QUERY_POINT = (554, 306)

    matcher = DINOv3Matcher(REPO_DIR, MODEL_PATH)
    matcher.find_correspondence(IMG1, IMG2, QUERY_POINT)