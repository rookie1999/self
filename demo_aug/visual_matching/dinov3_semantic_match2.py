import sys
import os
import torch
import torch.nn.functional as F
import matplotlib
import numpy as np

# å¼ºåˆ¶ä½¿ç”¨éäº¤äº’å¼åç«¯
matplotlib.use('TKAgg')
import matplotlib.pyplot as plt
from matplotlib.patches import Circle
from PIL import Image
from torchvision import transforms
from safetensors.torch import load_file

# TODO: Benson dinov3çš„ä»£ç æœ‰ç‚¹é—®é¢˜ï¼Œè¿˜å¯ä»¥è°ƒä¸€è°ƒ
class DINOv3Matcher:
    def __init__(self,
                 repo_dir,
                 model_path,
                 device='cuda' if torch.cuda.is_available() else 'cpu',
                 img_size=512):  # æ˜¾å¼æŒ‡å®šé«˜åˆ†è¾¨ç‡
        """
        DINOv3 è¯­ä¹‰åŒ¹é…å™¨ (èåˆä¼˜åŒ–ç‰ˆ)
        ä¼˜ç‚¹:
        1. æ”¯æŒ 512x512 é«˜åˆ†è¾¨ç‡ (é€šè¿‡ Pos Embed æ’å€¼)
        2. ä½¿ç”¨ä¸­é—´å±‚ç‰¹å¾èåˆ (è§£å†³ç‰¹å¾åç¼©ï¼Œæé«˜å‡ ä½•ç²¾åº¦)
        3. æ˜¾å­˜ä¼˜åŒ– (CPU å¤„ç†ç‰¹å¾å›¾)
        """
        self.device = device
        self.img_size = img_size
        self.patch_size = 16

        print(f"ğŸš€ åˆå§‹åŒ–... è®¾å¤‡: {self.device}, åˆ†è¾¨ç‡: {img_size}x{img_size}")

        # 1. åŠ è½½æ¶æ„ (å¼•ç”¨ä½ çš„æ–°å†™æ³•ï¼Œæ›´ç¨³å¥)
        self.model = self._load_architecture(repo_dir)
        self.model.to(self.device)
        self.model.eval()

        # 2. åŠ è½½æƒé‡ (åŒ…å« Pos Embed æ’å€¼)
        self._load_weights(model_path)

        # 3. é¢„å¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((self.img_size, self.img_size), interpolation=3),  # Bicubic
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ])

    def _load_architecture(self, repo_dir):
        if repo_dir not in sys.path:
            sys.path.append(repo_dir)

        # åŠ¨æ€å¯¼å…¥
        try:
            from dinov3.models.vision_transformer import vit_base
        except ImportError:
            print(f"âŒ æ— æ³•åœ¨ {repo_dir} æ‰¾åˆ° dinov3 æ¨¡å—ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
            raise

        print("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹æ¶æ„...")
        # å°è¯•å¼€å¯ registers (DINOv3 é»˜è®¤é…ç½®)
        try:
            model = vit_base(
                img_size=self.img_size,
                patch_size=16,
                num_register_tokens=4
            )
            self.has_registers = True
        except TypeError:
            print("âš ï¸ è­¦å‘Š: å½“å‰ä»£ç åº“ä¸æ”¯æŒ register_tokensï¼Œä½¿ç”¨æ ‡å‡† ViTã€‚")
            model = vit_base(img_size=self.img_size, patch_size=16)
            self.has_registers = False

        return model

    def _load_weights(self, model_path):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æƒé‡: {model_path}")

        print(f"ğŸ“¦ åŠ è½½å¹¶é€‚é…æƒé‡: {os.path.basename(model_path)}")
        state_dict = load_file(model_path)
        new_dict = {}
        model_params = dict(self.model.named_parameters())

        for k, v in state_dict.items():
            # === é”®åæ¸…æ´—æ˜ å°„ (æ¥è‡ªä½ çš„æ–°ä»£ç ) ===
            k = k.replace('module.', '').replace('backbone.', '')
            k = k.replace('embeddings.patch_embeddings.weight', 'patch_embed.proj.weight')
            k = k.replace('embeddings.patch_embeddings.bias', 'patch_embed.proj.bias')
            k = k.replace('embeddings.cls_token', 'cls_token')
            k = k.replace('embeddings.mask_token', 'mask_token')
            k = k.replace('embeddings.position_embeddings', 'pos_embed')
            k = k.replace('embeddings.register_tokens', 'register_tokens')
            k = k.replace('encoder.layers.', 'blocks.')
            k = k.replace('encoder.norm.', 'norm.')

            # å¤„ç† QKV æƒé‡èåˆ (å¦‚æœ safetensors é‡Œæ˜¯åˆ†å¼€çš„ï¼Œéœ€è¦åˆå¹¶)
            # æ³¨æ„ï¼šModelScope çš„ lvd1689m æƒé‡é€šå¸¸å·²ç»æ˜¯èåˆå¥½çš„ qkvï¼Œ
            # å¦‚æœæŠ¥é”™å°ºå¯¸ä¸åŒ¹é…ï¼Œå¯èƒ½éœ€è¦åœ¨è¿™é‡ŒåŠ  QKV åˆå¹¶é€»è¾‘ã€‚
            # æš‚æ—¶å‡è®¾ä½ çš„æƒé‡æ–‡ä»¶æ ¼å¼ä¸æ–°ä»£ç å‡è®¾çš„ä¸€è‡´ã€‚

            # === ç»´åº¦é€‚é… ===
            if k in model_params:
                target_shape = model_params[k].shape
                if v.ndim != len(target_shape):
                    if v.ndim == 2 and len(target_shape) == 3:
                        v = v.unsqueeze(1)
                    elif v.ndim == 3 and len(target_shape) == 2:
                        v = v.squeeze(1)

            # === å…³é”®ï¼šä½ç½®ç¼–ç æ’å€¼ ===
            if k == 'pos_embed' and v.shape != model_params[k].shape:
                print(f"ğŸ”„ Resizing pos_embed: {v.shape} -> {model_params[k].shape}")
                v = self._resize_pos_embed(v, model_params[k].shape)

            new_dict[k] = v

        msg = self.model.load_state_dict(new_dict, strict=False)
        print(f"âœ… æƒé‡åŠ è½½å®Œæ¯•. Missing keys (å¯å¿½ç•¥ head/rope): {len(msg.missing_keys)}")

    def _resize_pos_embed(self, pos_embed, expected_shape):
        """ä½ç½®ç¼–ç åŒçº¿æ€§æ’å€¼ (æ ¸å¿ƒä¿®å¤é€»è¾‘)"""
        n_special = 1 + (4 if self.has_registers else 0)  # CLS + Registers

        # åˆ†ç¦»ç‰¹æ®Š Token å’Œ Patch Token
        cls_tokens = pos_embed[:, :n_special, :]
        patch_tokens = pos_embed[:, n_special:, :]

        # Reshape æˆ 2D ç½‘æ ¼
        orig_num_patches = patch_tokens.shape[1]
        orig_size = int(orig_num_patches ** 0.5)  # e.g. 14
        dim = patch_tokens.shape[-1]

        patch_tokens = patch_tokens.reshape(1, orig_size, orig_size, dim).permute(0, 3, 1, 2)

        # è®¡ç®—ç›®æ ‡å°ºå¯¸
        target_num_patches = expected_shape[1] - n_special
        target_size = int(target_num_patches ** 0.5)  # e.g. 32

        # æ’å€¼
        patch_tokens = F.interpolate(
            patch_tokens, size=(target_size, target_size),
            mode='bicubic', align_corners=False
        )

        # å±•å¹³å› [1, N, D]
        patch_tokens = patch_tokens.permute(0, 2, 3, 1).flatten(1, 2)

        return torch.cat((cls_tokens, patch_tokens), dim=1)

    def extract_features(self, tensor):
        """
        æ”¹è¿›ç‰ˆç‰¹å¾æå–ï¼šä½¿ç”¨ä¸­é—´å±‚ + CPU Offload
        """
        with torch.inference_mode():
            # === æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ get_intermediate_layers ===
            # n=4: æå–æœ€å4å±‚ï¼Œå¢åŠ å‡ ä½•ç‰¹å¾ä¸°å¯Œåº¦
            # reshape=True: è‡ªåŠ¨å˜ä¸º [B, C, H, W]
            features_list = self.model.get_intermediate_layers(
                tensor,
                n=4,
                reshape=True
            )

            # æ‹¼æ¥å¤šå±‚ç‰¹å¾ [1, 768*4, 32, 32]
            # ç«‹å³è½¬åˆ° CPU é˜²æ­¢ OOM
            feature_map = torch.cat(features_list, dim=1).cpu()

        # ä¸Šé‡‡æ ·å› 512x512
        feature_map = F.interpolate(
            feature_map.float(),
            size=(self.img_size, self.img_size),
            mode='bicubic',
            align_corners=False
        )

        # å½’ä¸€åŒ–
        feature_map = F.normalize(feature_map, dim=1)

        return feature_map

    def find_correspondence(self, img1_path, img2_path, query_point):
        # 1. é¢„å¤„ç†
        img1_pil, tensor1 = self.preprocess(img1_path)
        img2_pil, tensor2 = self.preprocess(img2_path)

        # 2. æå–ç‰¹å¾ (æ­¤æ—¶å·²åœ¨ CPU)
        print("ğŸ§  æå–ç‰¹å¾ä¸­...")
        feat1 = self.extract_features(tensor1)
        feat2 = self.extract_features(tensor2)

        # 3. åæ ‡è½¬æ¢
        orig_w, orig_h = img1_pil.size
        qx, qy = query_point

        # æ˜ å°„åˆ° 512x512 ç©ºé—´
        x_512 = int(qx / orig_w * self.img_size)
        y_512 = int(qy / orig_h * self.img_size)

        # è¾¹ç•Œä¿æŠ¤
        x_512 = min(max(x_512, 0), self.img_size - 1)
        y_512 = min(max(y_512, 0), self.img_size - 1)

        # 4. åŒ¹é…è®¡ç®— (CPU)
        # è·å–æºç‚¹ç‰¹å¾å‘é‡ [C]
        target_vec = feat1[0, :, y_512, x_512]

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        # [1, C, H, W] * [C] -> [1, H, W]
        sim_map = torch.einsum('bchw, c -> bhw', feat2, target_vec)

        sim_map_np = sim_map[0].numpy()
        best_idx_flat = sim_map_np.argmax()
        y_max_512, x_max_512 = np.unravel_index(best_idx_flat, sim_map_np.shape)
        max_sim = sim_map_np[y_max_512, x_max_512]

        # 5. è¿˜åŸåæ ‡
        target_orig_w, target_orig_h = img2_pil.size
        target_x = int(x_max_512 / self.img_size * target_orig_w)
        target_y = int(y_max_512 / self.img_size * target_orig_h)

        print(f"ğŸ¯ åŒ¹é…æˆåŠŸ: ({target_x}, {target_y}), ç›¸ä¼¼åº¦: {max_sim:.3f}")
        self._visualize(img1_pil, img2_pil, query_point, (target_x, target_y))

    def preprocess(self, img_path):
        image = Image.open(img_path).convert('RGB')
        tensor = self.transform(image).unsqueeze(0).to(self.device)
        return image, tensor

    def _visualize(self, img1, img2, pt1, pt2):
        fig, axes = plt.subplots(1, 2, figsize=(16, 8))
        axes[0].imshow(img1)
        axes[0].add_patch(Circle(pt1, 8, color='red', fill=True))
        axes[0].set_title(f"Source {pt1}")

        axes[1].imshow(img2)
        axes[1].add_patch(Circle(pt2, 8, color='red', fill=True))
        axes[1].set_title(f"Target {pt2}")

        plt.tight_layout()
        plt.show()


if __name__ == "__main__":
    # é…ç½®åŒº
    REPO_DIR = "/home/benson/projects/dinov3"
    # è¯·ç¡®ä¿è¿™é‡Œæ˜¯ä½ çš„ safetensors æƒé‡è·¯å¾„
    MODEL_PATH = "/home/benson/projects/second_work/modelscope/hub/models/facebook/dinov3-vitb16-pretrain-lvd1689m/model.safetensors"

    IMG1 = "pic.jpg"
    IMG2 = "pic2.jpg"
    QUERY_POINT = (554, 306)  # æºå›¾ä¸Šçš„ç‚¹

    if not os.path.exists(IMG1):
        print("âš ï¸ ç”Ÿæˆæµ‹è¯•å›¾ç‰‡...")
        Image.new('RGB', (600, 800), 'white').save(IMG1)
        Image.new('RGB', (700, 600), 'gray').save(IMG2)

    matcher = DINOv3Matcher(REPO_DIR, MODEL_PATH)
    matcher.find_correspondence(IMG1, IMG2, QUERY_POINT)